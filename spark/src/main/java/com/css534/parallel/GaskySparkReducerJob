import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.Function;
import java.util.Collections;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Comparator;
import java.util.Iterator;
import java.util.List;
import com.google.common.collect.Lists;
import com.google.common.collect.Iterables;
import com.google.common.collect.Lists;

public class GaskySparkJob {
    public static void main(String[] args) {
        // Create a Spark context
        SparkConf conf = new SparkConf().setAppName("GaskySparkReducerJob");
        JavaSparkContext sparkContext = new JavaSparkContext(conf);

        // Load input data
        JavaRDD<String> inputData = sparkContext.textFile(args[0]);

        // Debug: Print the input data
        System.out.println("Debug: Input Data:");
        inputData.foreach(line -> System.out.println(line));

        // Process the input data and create key-value pairs
        JavaPairRDD<Tuple2<String, Integer>, Iterable<Tuple2<Integer, Double>>> result = inputData
                .flatMapToPair(line -> parseInputData(line))
                .groupByKey();

        // Collect the result and sort it
        List<Tuple2<Tuple2<String, Integer>, Iterable<Tuple2<Integer, Double>>>> resultList = result.collect();

        // Create a new list with the sorted elements
        List<Tuple2<Tuple2<String, Integer>, Iterable<Tuple2<Integer, Double>>>> sortedResultList = new ArrayList<>(resultList);
        sortedResultList.sort(Comparator.comparing(
                (Tuple2<Tuple2<String, Integer>, Iterable<Tuple2<Integer, Double>>> tuple) -> tuple._1(),
                Tuple2Comparator.INSTANCE
        ));

        // Separate keys and values from the sorted result
        List<Tuple2<String, Integer>> keysList = new ArrayList<>();
        List<Iterable<Tuple2<Integer, Double>>> valuesList = new ArrayList<>();

        for (Tuple2<Tuple2<String, Integer>, Iterable<Tuple2<Integer, Double>>> tuple : sortedResultList) {
            keysList.add(tuple._1());
            valuesList.add(tuple._2());
        }

        // Example: Print keys and values
        System.out.println("Keys:");
        keysList.forEach(key -> System.out.println(key));

        System.out.println("Values:");
        valuesList.forEach(values -> {
            values.forEach(value -> System.out.print("  " + value));
            System.out.println("");
        });

        // Call the new method to apply gaskyAlgorithm
        applyGaskyAlgorithm(sparkContext,keysList, valuesList);

        // Stop the Spark context
        sparkContext.stop();
    }

    // New method to apply gaskyAlgorithm to each tuple along with its corresponding key
    private static void applyGaskyAlgorithm(JavaSparkContext sparkContext,List<Tuple2<String, Integer>> keysList, List<Iterable<Tuple2<Integer, Double>>> valuesList) {
        System.out.println("valuesList size"+valuesList.size());
        for (int i = 0; i < valuesList.size(); i++) {
            Tuple2<String, Integer> keysTuple = keysList.get(i);
            int colNumber = keysTuple._2(); 
            Iterable<Tuple2<Integer, Double>> valuesTuple = valuesList.get(i);
                    
            
            System.out.println("Column number"+colNumber);
            // Assuming gaskyAlgorithm takes an Iterable<Tuple2<Integer, Double>> and a Tuple2<String, Integer>
            gaskyAlgorithm(sparkContext, valuesTuple, colNumber);
        }
    }

    private static void gaskyAlgorithm(JavaSparkContext sparkContext, Iterable<Tuple2<Integer, Double>> values, int colNumber) {
    int totalPoints = Iterables.size(values);
    List<Double> distances = new ArrayList<>(Collections.nCopies(totalPoints, Double.MAX_VALUE));
    System.out.println("Total points in Cartesian: " + totalPoints);

    if (totalPoints > 2) {
        // Convert Iterable to a List
        List<Tuple2<Integer, Double>> points = Lists.newArrayList(values);

        // Use Spark transformations to filter points based on dominance
        JavaPairRDD<Integer, Double> filteredPointsRDD = sparkContext.parallelize(points)
                .mapToPair(point -> new Tuple2<>(point._1().doubleValue(), calcBisectorProjections(point._1().doubleValue(), point._2())))
                .reduceByKey((point1, point2) -> ((Double)point1) > ((Double)point2) ? point1 : point2)
                .values();

        // Collect the results back to the driver
        List<Tuple2<Integer, Double>> filteredPoints = filteredPointsRDD.collect();

        System.out.println("The current remained dominated points are " + filteredPoints.size());
        List<double[]> proximityProjectionsPoints = findProximityPoints(sparkContext, Lists.newArrayList(filteredPoints), totalPoints);
    }
}


    // FlatMap function to process input data and generate key-value pairs
    private static Iterator<Tuple2<Tuple2<String, Integer>, Tuple2<Integer, Double>>> parseInputData(String line) {
        System.out.println("Debug: Processing Line: " + line);

        String[] distFavArray = line.split("\\s+");
        List<Tuple2<Tuple2<String, Integer>, Tuple2<Integer, Double>>> result = new ArrayList<>();

        if (distFavArray.length > 0) {
            String facilityName = distFavArray[0];
            int matrixRowNumber = Integer.parseInt(distFavArray[1]);

            // Convert the strings to a list of strings
            List<String> binMatrixValues = Arrays.asList(Arrays.copyOfRange(distFavArray, 2, distFavArray.length));

            double[] leftDistance = new double[binMatrixValues.get(0).length()];
            double[] rightDistance = new double[binMatrixValues.get(0).length()];

            Arrays.fill(leftDistance, Double.MAX_VALUE);
            Arrays.fill(rightDistance, Double.MAX_VALUE);

            leftDistance = getLeftDistance(leftDistance, binMatrixValues);
            rightDistance = getRightDistance(rightDistance, binMatrixValues);

            for (int i = 0; i < binMatrixValues.get(0).length(); i++) {
                result.add(new Tuple2<>(new Tuple2<>(facilityName, i + 1),
                        new Tuple2<>(matrixRowNumber, Double.min(leftDistance[i], rightDistance[i]))));
            }
        }

        // Debug: Print the generated key-value pairs
        System.out.println("Debug: Generated Key-Value Pairs:");
        result.forEach(tuple -> System.out.println(tuple));

        return result.iterator();
    }

    private static double[] getLeftDistance(double[] leftDistance, List<String> gridRows) {
        boolean isFavlFound = false;
        for (int i = 0; i < gridRows.get(0).length(); i++) {
            if (gridRows.get(0).charAt(i) == '1') {
                leftDistance[i] = 0;
                isFavlFound = true;
            } else if (isFavlFound) {
                leftDistance[i] = leftDistance[i - 1] + 1;
            }
        }
        return leftDistance;
    }

    private static double[] getRightDistance(double[] rightDistance, List<String> gridRows) {
        boolean isFavrFound = false;
        for (int i = gridRows.get(0).length() - 1; i >= 0; --i) {
            if (gridRows.get(0).charAt(i) == '1') {
                rightDistance[i] = 0;
                isFavrFound = true;
            } else if (isFavrFound) {
                rightDistance[i] = rightDistance[i + 1] + 1;
            }
        }
        return rightDistance;
    }

    // Comparator for Tuple2<String, Integer>
    static class Tuple2Comparator implements Comparator<Tuple2<String, Integer>>, Serializable {
        static final Tuple2Comparator INSTANCE = new Tuple2Comparator();

        @Override
        public int compare(Tuple2<String, Integer> tuple1, Tuple2<String, Integer> tuple2) {
            int compareResult = tuple1._1().compareTo(tuple2._1());
            if (compareResult == 0) {
                // If the first elements are equal, compare the second elements
                compareResult = Integer.compare(tuple1._2(), tuple2._2());
            }
            return compareResult;
        }
    }


    private static Tuple2<Double, Double> calcBisectorProjections(Double x1, Double y1){
        double x = x1;
        double y = y1;
        double xx = ((y * y) - (y * y) + (x * x) - (x * x)) / (2 * (y - x));
        double yy = 0;
        return new Tuple2<>(xx, yy);
    }

    private static List<double[]> findProximityPoints(JavaSparkContext sparkContext,
                                                  List<Tuple2<Double, Double>> unDominatedPoints,
                                                  final int totalPoints) {
    // Create a JavaRDD from the list of unDominatedPoints
    JavaRDD<Tuple2<Double, Double>> unDominatedPointsRDD = sparkContext.parallelize(unDominatedPoints);

    // Calculate intervals between consecutive points
    JavaRDD<Tuple2<Double, Double>> intervalsRDD = unDominatedPointsRDD
            .zipWithIndex()
            .filter(tuple -> tuple._2() > 0) // Exclude the first point
            .map(tuple -> {
                Tuple2<Double, Double> point1 = unDominatedPoints.get(tuple._2().intValue() - 1);
                Tuple2<Double, Double> point2 = tuple._1();

                return new Tuple2<>(
                        (point1._1() + point2._1()) / 2,
                        0.0 // point lying on with intersection on X axis
                );
            });

    // Combine intervals
    JavaPairRDD<Double, Double> mergedIntervalRDD = intervalsRDD
            .mapToPair(interval -> new Tuple2<>(interval._1(), interval._1()))
            .reduceByKey((interval1, interval2) -> new Tuple2<>(interval1, interval2));

    // Collect the results back to the driver
    List<Tuple2<Double, Double>> mergedIntervals = mergedIntervalRDD.collect();

    // Convert List<Tuple2<Double, Double>> to List<double[]>
    List<double[]> mergedIntervalList = new ArrayList<>();
    for (Tuple2<Double, Double> interval : mergedIntervals) {
        mergedIntervalList.add(new double[]{interval._1(), totalPoints});
    }

    return mergedIntervalList;
}


}
